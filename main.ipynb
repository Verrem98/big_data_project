{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8779ed80-afb9-4d06-983d-0c9fd9d6ec39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.597670900Z",
     "start_time": "2025-03-03T23:30:20.596670700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e836b3e0-589e-4494-bb4d-7ef3e4960d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.598671300Z",
     "start_time": "2025-03-03T23:30:20.597670900Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    \n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b723323f-19d4-4c4e-8a66-19b03a8536f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.610674500Z",
     "start_time": "2025-03-03T23:30:20.598671300Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    text = unidecode(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a57a8b-a232-4fbd-be4d-cc9af68d86b2",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.599672Z"
    }
   },
   "outputs": [],
   "source": [
    "root = 'reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8c1d01-8fbd-4346-b7b6-c5a7283dc784",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.600672200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in [1,2,3,4]], ignore_index=True)\n",
    "test_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in [7]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0699e1-006d-4c7d-9e0d-0493563b5656",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_15324\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_15324\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "train_data['review_body'] = train_data['review_body'].apply(pre_process)\n",
    "test_data['review_body'] = test_data['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dca1e4-db0e-4b86-95dd-c664ab8409e1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_15324\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "validation_hidden_df = pd.read_csv(root + 'validation_hidden.csv')\n",
    "test_hidden_df = pd.read_csv(root + 'test_hidden.csv')\n",
    "\n",
    "validation_hidden_df['review_body'] = validation_hidden_df['review_body'].apply(pre_process)\n",
    "test_hidden_df['review_body'] = test_hidden_df['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0cc0c9-855b-405e-9b10-e9d0c635cc0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.603672800Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b40453b-bead-464a-8675-970836275b11",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.605673300Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "\n",
    "        with open(root + \"category.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        df['category_name'] = df['product_category_id'].apply(lambda x: {d['id']:d['name'].replace(\"_\",' ').lower() for d in data}[x])\n",
    "\n",
    "        \n",
    "        texts = [str(df.category_name.tolist()[i]) + ' '+ str(df.review_headline.tolist()[i]) + ' ' + str(df.category_name.tolist()[i]) + ' ' + str(df.review_body.tolist()[i])\n",
    "            for i in range(len(df))\n",
    "        ]\n",
    "        self.encodings = tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if 'label' in df.columns.tolist():\n",
    "            self.labels = torch.tensor(df.label.tolist(), dtype=torch.float)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"label\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6384ecd3-ae66-48d7-96fe-44555e36df7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerForBinaryClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(TransformerForBinaryClassification, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.text_classifier = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            pooled_output = (hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        logits = self.text_classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98244d2-e9a9-4dbc-b3da-c5fa1c87041e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device).float() \n",
    "\n",
    "            logits = model(input_ids, attention_mask).view(-1) \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c8900-7deb-43b9-a941-28b3cd401c1f",
   "metadata": {},
   "source": [
    "# Finetune the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721e8d5b-ddb8-4605-8a51-028a8af9fb9f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.608674400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 611/611 [01:09<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.577, Train Accuracy = 0.699\n",
      "Epoch 1: Test Loss = 0.577, Test Accuracy = 0.722\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Test Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(test_loss,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(test_accuracy,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/bert_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_accuracy \u001b[38;5;241m>\u001b[39m best_test_accuracy:\n\u001b[0;32m     63\u001b[0m     best_test_accuracy \u001b[38;5;241m=\u001b[39m test_accuracy\n",
      "File \u001b[1;32m~\\.conda\\envs\\DSP-project\\Lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DSP-project\\Lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\DSP-project\\Lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "#model_name = 'google-bert/bert-base-multilingual-cased'\n",
    "#model_name = 'FacebookAI/xlm-roberta-base'\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "#model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" \n",
    "#model_name = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TransformerForBinaryClassification(model_name)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "test_dataset = TextDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "best_test_accuracy = 0.0 \n",
    "best_model_path = \"models/best_model.pth\"  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predictions = torch.sigmoid(logits).view(-1)\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {round(train_loss,3)}, Train Accuracy = {round(train_accuracy,3)}\")\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: Test Loss = {round(test_loss,3)}, Test Accuracy = {round(test_accuracy,3)}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/bert_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3d237-3914-4eff-a27a-ce32efe66c36",
   "metadata": {},
   "source": [
    "# Generate the files for online submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6234a8-d42b-4250-95c2-2c7496da3f87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.609674300Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_predictions_csv(df, filename, model, tokenizer, device):\n",
    "    dataset = TextDataset(df, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {filename}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).view(-1)\n",
    "            preds = (probs > 0.5).tolist()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions) \n",
    "    df_predictions.to_csv(filename, index=False, header=False)\n",
    "\n",
    "generate_predictions_csv(validation_hidden_df, \"validation_hidden.csv\", model, tokenizer, device)\n",
    "generate_predictions_csv(test_hidden_df, \"test_hidden.csv\", model, tokenizer, device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
