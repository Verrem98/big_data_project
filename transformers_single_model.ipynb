{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8779ed80-afb9-4d06-983d-0c9fd9d6ec39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.597670900Z",
     "start_time": "2025-03-03T23:30:20.596670700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e836b3e0-589e-4494-bb4d-7ef3e4960d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.598671300Z",
     "start_time": "2025-03-03T23:30:20.597670900Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    \n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b723323f-19d4-4c4e-8a66-19b03a8536f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.610674500Z",
     "start_time": "2025-03-03T23:30:20.598671300Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    text = unidecode(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a57a8b-a232-4fbd-be4d-cc9af68d86b2",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.599672Z"
    }
   },
   "outputs": [],
   "source": [
    "root = 'reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8c1d01-8fbd-4346-b7b6-c5a7283dc784",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.600672200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in range(6)], ignore_index=True)\n",
    "test_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in [6,7]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0699e1-006d-4c7d-9e0d-0493563b5656",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_11292\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_11292\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "train_data['review_body'] = train_data['review_body'].apply(pre_process)\n",
    "test_data['review_body'] = test_data['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dca1e4-db0e-4b86-95dd-c664ab8409e1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_11292\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "validation_hidden_df = pd.read_csv(root + 'validation_hidden.csv')\n",
    "test_hidden_df = pd.read_csv(root + 'test_hidden.csv')\n",
    "\n",
    "validation_hidden_df['review_body'] = validation_hidden_df['review_body'].apply(pre_process)\n",
    "test_hidden_df['review_body'] = test_hidden_df['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0cc0c9-855b-405e-9b10-e9d0c635cc0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.603672800Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b40453b-bead-464a-8675-970836275b11",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.605673300Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "\n",
    "        with open(root + \"category.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        df['category_name'] = df['product_category_id'].apply(lambda x: {d['id']:d['name'].replace(\"_\",' ').lower() for d in data}[x])\n",
    "\n",
    "        \n",
    "        texts = [str(df.category_name.tolist()[i]) + ' '+ str(df.review_headline.tolist()[i]) + ' ' + str(df.category_name.tolist()[i]) + ' ' + str(df.review_body.tolist()[i])\n",
    "            for i in range(len(df))\n",
    "        ]\n",
    "        self.encodings = tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if 'label' in df.columns.tolist():\n",
    "            self.labels = torch.tensor(df.label.tolist(), dtype=torch.float)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"label\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6384ecd3-ae66-48d7-96fe-44555e36df7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerForBinaryClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(TransformerForBinaryClassification, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.text_classifier = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            pooled_output = (hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        logits = self.text_classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98244d2-e9a9-4dbc-b3da-c5fa1c87041e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device).float() \n",
    "\n",
    "            logits = model(input_ids, attention_mask).view(-1) \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c8900-7deb-43b9-a941-28b3cd401c1f",
   "metadata": {},
   "source": [
    "# Finetune the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721e8d5b-ddb8-4605-8a51-028a8af9fb9f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.608674400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   7%|██████████████████▏                                                                                                                                                                                                                                             | 65/914 [00:46<10:07,  1.40it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'google-bert/bert-base-multilingual-cased'\n",
    "#model_name = 'FacebookAI/xlm-roberta-base'\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "#model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" \n",
    "#model_name = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TransformerForBinaryClassification(model_name)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "test_dataset = TextDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "best_test_accuracy = 0.0 \n",
    "best_model_path = \"models/best_model.pth\"  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predictions = torch.sigmoid(logits).view(-1)\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {round(train_loss,3)}, Train Accuracy = {round(train_accuracy,3)}\")\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}: Test Loss = {round(test_loss,3)}, Test Accuracy = {round(test_accuracy,3)}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"models/bert_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3d237-3914-4eff-a27a-ce32efe66c36",
   "metadata": {},
   "source": [
    "# Generate the files for online submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6234a8-d42b-4250-95c2-2c7496da3f87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.609674300Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_predictions_csv(df, filename, model, tokenizer, device):\n",
    "    dataset = TextDataset(df, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {filename}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).view(-1)\n",
    "            preds = (probs > 0.5).tolist()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions) \n",
    "    df_predictions.to_csv(filename, index=False, header=False)\n",
    "\n",
    "generate_predictions_csv(validation_hidden_df, \"validation_hidden.csv\", model, tokenizer, device)\n",
    "generate_predictions_csv(test_hidden_df, \"test_hidden.csv\", model, tokenizer, device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
