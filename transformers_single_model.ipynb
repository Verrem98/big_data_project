{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8779ed80-afb9-4d06-983d-0c9fd9d6ec39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.597670900Z",
     "start_time": "2025-03-03T23:30:20.596670700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e836b3e0-589e-4494-bb4d-7ef3e4960d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.598671300Z",
     "start_time": "2025-03-03T23:30:20.597670900Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    \n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b723323f-19d4-4c4e-8a66-19b03a8536f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T23:30:20.610674500Z",
     "start_time": "2025-03-03T23:30:20.598671300Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    text = unidecode(text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a57a8b-a232-4fbd-be4d-cc9af68d86b2",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.599672Z"
    }
   },
   "outputs": [],
   "source": [
    "root = 'reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8c1d01-8fbd-4346-b7b6-c5a7283dc784",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.600672200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in range(5)], ignore_index=True)\n",
    "test_data = pd.concat([pd.read_csv(root + f\"train-{i+1}.csv\") for i in [6,7]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0699e1-006d-4c7d-9e0d-0493563b5656",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_9088\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_9088\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "train_data['review_body'] = train_data['review_body'].apply(pre_process)\n",
    "test_data['review_body'] = test_data['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dca1e4-db0e-4b86-95dd-c664ab8409e1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.602672600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_9088\\1899272772.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "validation_hidden_df = pd.read_csv(root + 'validation_hidden.csv')\n",
    "test_hidden_df = pd.read_csv(root + 'test_hidden.csv')\n",
    "\n",
    "validation_hidden_df['review_body'] = validation_hidden_df['review_body'].apply(pre_process)\n",
    "test_hidden_df['review_body'] = test_hidden_df['review_body'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0cc0c9-855b-405e-9b10-e9d0c635cc0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.603672800Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f4a700a-8ef9-459a-b5b7-3b0133cca89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>marketplace_id</th>\n",
       "      <th>product_category_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>B001N2MZT8</td>\n",
       "      <td>903886718</td>\n",
       "      <td>Green Zone [DVD]</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>green zone</td>\n",
       "      <td>i found at first it was a little difficult to ...</td>\n",
       "      <td>2010-11-15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>B00GCBVE0Q</td>\n",
       "      <td>282740618</td>\n",
       "      <td>Le secret de Green Knowe</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>j'ai aime cette histoire. les acteurs et surto...</td>\n",
       "      <td>2014-11-23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>1423165691</td>\n",
       "      <td>883799517</td>\n",
       "      <td>A Disney Sketchbook.</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>okay mais...</td>\n",
       "      <td>estce une coincidence que la plupart des princ...</td>\n",
       "      <td>2012-12-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0061091480</td>\n",
       "      <td>623343977</td>\n",
       "      <td>Your Erroneous Zones</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Arrogant</td>\n",
       "      <td>wayne dyer is a popular american personal grow...</td>\n",
       "      <td>2009-07-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>B00HZ4CYOY</td>\n",
       "      <td>647510225</td>\n",
       "      <td>König der Mathematik Junior</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Tớllé Máthé Ápp...</td>\n",
       "      <td>.....unsere kids mogen diese art des lernens. ...</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>11976</td>\n",
       "      <td>B009R9WAZ8</td>\n",
       "      <td>780895978</td>\n",
       "      <td>Unapologetic</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>fan !!</td>\n",
       "      <td>tout d'abord, je precise que mon commentaire n...</td>\n",
       "      <td>2013-07-17</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>11988</td>\n",
       "      <td>B000FO8968</td>\n",
       "      <td>635536816</td>\n",
       "      <td>Star Wars: Episode IV - Eine neue Hoffnung (Or...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Fast Perfekt</td>\n",
       "      <td>also ich muss sagen ich war anfangs sehr skept...</td>\n",
       "      <td>2007-05-24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>11989</td>\n",
       "      <td>B0006V4F1I</td>\n",
       "      <td>725126142</td>\n",
       "      <td>Kill Bill 1 and 2 (Box Set) [DVD]</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Nice but simply Kill Bill 1+2. No extras</td>\n",
       "      <td>if you are looking for a collector's box with ...</td>\n",
       "      <td>2011-02-08</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>11992</td>\n",
       "      <td>B0031R5K72</td>\n",
       "      <td>47504452</td>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>good film good book</td>\n",
       "      <td>2014-12-07</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>11999</td>\n",
       "      <td>B0095HHN4A</td>\n",
       "      <td>100047036</td>\n",
       "      <td>The Hobbit: The Battle of the Five Armies [Blu...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>wow what a film and what a trilogy</td>\n",
       "      <td>2015-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6082 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  product_id  product_parent  \\\n",
       "0              9  B001N2MZT8       903886718   \n",
       "1             11  B00GCBVE0Q       282740618   \n",
       "2             19  1423165691       883799517   \n",
       "3             33  0061091480       623343977   \n",
       "4             34  B00HZ4CYOY       647510225   \n",
       "...          ...         ...             ...   \n",
       "6077       11976  B009R9WAZ8       780895978   \n",
       "6078       11988  B000FO8968       635536816   \n",
       "6079       11989  B0006V4F1I       725126142   \n",
       "6080       11992  B0031R5K72        47504452   \n",
       "6081       11999  B0095HHN4A       100047036   \n",
       "\n",
       "                                          product_title vine  \\\n",
       "0                                      Green Zone [DVD]    N   \n",
       "1                              Le secret de Green Knowe    N   \n",
       "2                                  A Disney Sketchbook.    N   \n",
       "3                                  Your Erroneous Zones    N   \n",
       "4                           König der Mathematik Junior    N   \n",
       "...                                                 ...  ...   \n",
       "6077                                       Unapologetic    N   \n",
       "6078  Star Wars: Episode IV - Eine neue Hoffnung (Or...    N   \n",
       "6079                  Kill Bill 1 and 2 (Box Set) [DVD]    N   \n",
       "6080                                     The Book Thief    N   \n",
       "6081  The Hobbit: The Battle of the Five Armies [Blu...    N   \n",
       "\n",
       "     verified_purchase                           review_headline  \\\n",
       "0                    Y                                green zone   \n",
       "1                    Y                                       NaN   \n",
       "2                    N                              okay mais...   \n",
       "3                    N                                  Arrogant   \n",
       "4                    Y                        Tớllé Máthé Ápp...   \n",
       "...                ...                                       ...   \n",
       "6077                 Y                                    fan !!   \n",
       "6078                 N                              Fast Perfekt   \n",
       "6079                 N  Nice but simply Kill Bill 1+2. No extras   \n",
       "6080                 Y                                Five Stars   \n",
       "6081                 Y                                Five Stars   \n",
       "\n",
       "                                            review_body review_date  \\\n",
       "0     i found at first it was a little difficult to ...  2010-11-15   \n",
       "1     j'ai aime cette histoire. les acteurs et surto...  2014-11-23   \n",
       "2     estce une coincidence que la plupart des princ...  2012-12-22   \n",
       "3     wayne dyer is a popular american personal grow...  2009-07-21   \n",
       "4     .....unsere kids mogen diese art des lernens. ...  2015-06-01   \n",
       "...                                                 ...         ...   \n",
       "6077  tout d'abord, je precise que mon commentaire n...  2013-07-17   \n",
       "6078  also ich muss sagen ich war anfangs sehr skept...  2007-05-24   \n",
       "6079  if you are looking for a collector's box with ...  2011-02-08   \n",
       "6080                                good film good book  2014-12-07   \n",
       "6081                 wow what a film and what a trilogy  2015-05-24   \n",
       "\n",
       "      marketplace_id  product_category_id  label  \n",
       "0                  1                    3  False  \n",
       "1                  2                    3  False  \n",
       "2                  0                    0  False  \n",
       "3                  0                    0   True  \n",
       "4                  0                    1  False  \n",
       "...              ...                  ...    ...  \n",
       "6077               2                    6   True  \n",
       "6078               0                    3   True  \n",
       "6079               1                    3   True  \n",
       "6080               1                   10  False  \n",
       "6081               1                    3  False  \n",
       "\n",
       "[6082 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b40453b-bead-464a-8675-970836275b11",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.605673300Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "\n",
    "        with open(root + \"category.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        df['category_name'] = df['product_category_id'].apply(lambda x: {d['id']:d['name'].replace(\"_\",' ').lower() for d in data}[x])\n",
    "\n",
    "        \n",
    "        texts = [\n",
    "        str(df.category_name.tolist()[i]) + ' ' + \n",
    "        str(df.review_headline.tolist()[i]) + ' ' + \n",
    "        str(df.category_name.tolist()[i]) + ' ' + \n",
    "        (\"vine \" if df.vine.tolist()[i] == 'Y' else '') + \n",
    "        (\"verified \" if df.verified_purchase.tolist()[i] == 'Y' else '') +\n",
    "        str(df.review_body.tolist()[i])\n",
    "        for i in range(len(df))\n",
    "        ]\n",
    "\n",
    "        \n",
    "        self.encodings = tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if 'label' in df.columns.tolist():\n",
    "            self.labels = torch.tensor(df.label.tolist(), dtype=torch.float)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"label\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6384ecd3-ae66-48d7-96fe-44555e36df7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerForBinaryClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(TransformerForBinaryClassification, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.text_classifier = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            pooled_output = (hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        logits = self.text_classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98244d2-e9a9-4dbc-b3da-c5fa1c87041e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.606673400Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device).float() \n",
    "\n",
    "            logits = model(input_ids, attention_mask).view(-1) \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c8900-7deb-43b9-a941-28b3cd401c1f",
   "metadata": {},
   "source": [
    "# Finetune the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "721e8d5b-ddb8-4605-8a51-028a8af9fb9f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.608674400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: Train Loss = 0.622, Train Accuracy = 0.657\n",
      "Step 50: Test Loss = 0.589, Test Accuracy = 0.689\n",
      "Step 100: Train Loss = 0.587, Train Accuracy = 0.675\n",
      "Step 100: Test Loss = 0.582, Test Accuracy = 0.694\n",
      "Step 150: Train Loss = 0.534, Train Accuracy = 0.743\n",
      "Step 150: Test Loss = 0.594, Test Accuracy = 0.708\n",
      "Step 200: Train Loss = 0.587, Train Accuracy = 0.695\n",
      "Step 200: Test Loss = 0.583, Test Accuracy = 0.68\n",
      "Step 250: Train Loss = 0.539, Train Accuracy = 0.73\n",
      "Step 250: Test Loss = 0.593, Test Accuracy = 0.691\n",
      "Step 300: Train Loss = 0.556, Train Accuracy = 0.715\n",
      "Step 300: Test Loss = 0.566, Test Accuracy = 0.71\n",
      "Step 350: Train Loss = 0.536, Train Accuracy = 0.743\n",
      "Step 350: Test Loss = 0.567, Test Accuracy = 0.707\n",
      "Step 400: Train Loss = 0.563, Train Accuracy = 0.743\n",
      "Step 400: Test Loss = 0.573, Test Accuracy = 0.706\n",
      "Step 450: Train Loss = 0.572, Train Accuracy = 0.69\n",
      "Step 450: Test Loss = 0.57, Test Accuracy = 0.712\n",
      "Step 500: Train Loss = 0.521, Train Accuracy = 0.745\n",
      "Step 500: Test Loss = 0.58, Test Accuracy = 0.694\n",
      "Step 550: Train Loss = 0.535, Train Accuracy = 0.73\n",
      "Step 550: Test Loss = 0.569, Test Accuracy = 0.712\n",
      "Step 600: Train Loss = 0.573, Train Accuracy = 0.72\n",
      "Step 600: Test Loss = 0.559, Test Accuracy = 0.712\n",
      "Step 650: Train Loss = 0.541, Train Accuracy = 0.718\n",
      "Step 650: Test Loss = 0.561, Test Accuracy = 0.716\n",
      "Step 700: Train Loss = 0.552, Train Accuracy = 0.72\n",
      "Step 700: Test Loss = 0.562, Test Accuracy = 0.714\n",
      "Step 750: Train Loss = 0.554, Train Accuracy = 0.72\n",
      "Step 750: Test Loss = 0.559, Test Accuracy = 0.71\n",
      "Step 800: Train Loss = 0.543, Train Accuracy = 0.726\n",
      "Step 800: Test Loss = 0.553, Test Accuracy = 0.72\n",
      "Step 850: Train Loss = 0.5, Train Accuracy = 0.752\n",
      "Step 850: Test Loss = 0.564, Test Accuracy = 0.709\n",
      "Step 900: Train Loss = 0.541, Train Accuracy = 0.725\n",
      "Step 900: Test Loss = 0.555, Test Accuracy = 0.717\n",
      "Step 950: Train Loss = 0.509, Train Accuracy = 0.745\n",
      "Step 950: Test Loss = 0.579, Test Accuracy = 0.7\n",
      "Step 1000: Train Loss = 0.514, Train Accuracy = 0.75\n",
      "Step 1000: Test Loss = 0.595, Test Accuracy = 0.692\n",
      "Step 1050: Train Loss = 0.533, Train Accuracy = 0.718\n",
      "Step 1050: Test Loss = 0.567, Test Accuracy = 0.704\n",
      "Step 1100: Train Loss = 0.533, Train Accuracy = 0.71\n",
      "Step 1100: Test Loss = 0.56, Test Accuracy = 0.718\n",
      "Step 1150: Train Loss = 0.503, Train Accuracy = 0.757\n",
      "Step 1150: Test Loss = 0.581, Test Accuracy = 0.71\n",
      "Step 1200: Train Loss = 0.516, Train Accuracy = 0.745\n",
      "Step 1200: Test Loss = 0.554, Test Accuracy = 0.723\n",
      "Step 1250: Train Loss = 0.485, Train Accuracy = 0.76\n",
      "Step 1250: Test Loss = 0.56, Test Accuracy = 0.723\n",
      "Step 1300: Train Loss = 0.523, Train Accuracy = 0.743\n",
      "Step 1300: Test Loss = 0.568, Test Accuracy = 0.708\n",
      "Step 1350: Train Loss = 0.522, Train Accuracy = 0.738\n",
      "Step 1350: Test Loss = 0.556, Test Accuracy = 0.72\n",
      "Step 1400: Train Loss = 0.573, Train Accuracy = 0.708\n",
      "Step 1400: Test Loss = 0.571, Test Accuracy = 0.712\n",
      "Step 1450: Train Loss = 0.503, Train Accuracy = 0.75\n",
      "Step 1450: Test Loss = 0.559, Test Accuracy = 0.715\n",
      "Step 1500: Train Loss = 0.518, Train Accuracy = 0.752\n",
      "Step 1500: Test Loss = 0.56, Test Accuracy = 0.713\n",
      "Step 1550: Train Loss = 0.523, Train Accuracy = 0.746\n",
      "Step 1550: Test Loss = 0.551, Test Accuracy = 0.721\n",
      "Step 1600: Train Loss = 0.475, Train Accuracy = 0.77\n",
      "Step 1600: Test Loss = 0.563, Test Accuracy = 0.729\n",
      "Step 1650: Train Loss = 0.469, Train Accuracy = 0.767\n",
      "Step 1650: Test Loss = 0.561, Test Accuracy = 0.726\n",
      "Step 1700: Train Loss = 0.495, Train Accuracy = 0.767\n",
      "Step 1700: Test Loss = 0.583, Test Accuracy = 0.698\n",
      "Step 1750: Train Loss = 0.39, Train Accuracy = 0.83\n",
      "Step 1750: Test Loss = 0.615, Test Accuracy = 0.692\n",
      "Step 1800: Train Loss = 0.413, Train Accuracy = 0.833\n",
      "Step 1800: Test Loss = 0.602, Test Accuracy = 0.722\n",
      "Step 1850: Train Loss = 0.438, Train Accuracy = 0.82\n",
      "Step 1850: Test Loss = 0.579, Test Accuracy = 0.704\n",
      "Step 1900: Train Loss = 0.495, Train Accuracy = 0.77\n",
      "Step 1900: Test Loss = 0.594, Test Accuracy = 0.672\n",
      "Step 1950: Train Loss = 0.461, Train Accuracy = 0.772\n",
      "Step 1950: Test Loss = 0.561, Test Accuracy = 0.712\n",
      "Step 2000: Train Loss = 0.458, Train Accuracy = 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TransformerForBinaryClassification(model_name)\n",
    "\n",
    "train_dataset = TextDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_steps = 10_000  \n",
    "best_test_accuracy = 0.0\n",
    "best_model_path = \"models/best_model.pth\"\n",
    "\n",
    "step = 0\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.train()\n",
    "train_iterator = iter(train_loader)  \n",
    "\n",
    "while step < max_steps:\n",
    "    try:\n",
    "        batch = next(train_iterator)  \n",
    "    except StopIteration:\n",
    "        train_iterator = iter(train_loader)  \n",
    "        batch = next(train_iterator)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids, attention_mask)\n",
    "\n",
    "    loss = criterion(logits.view(-1), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    predictions = torch.sigmoid(logits).view(-1)\n",
    "    predicted_labels = (predictions > 0.5).float()\n",
    "    correct += (predicted_labels == labels).sum().item()\n",
    "    total += labels.size(0)\n",
    "\n",
    "    step += 1\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        train_loss = total_loss / 100\n",
    "        train_accuracy = correct / total\n",
    "        print(f\"Step {step}: Train Loss = {round(train_loss, 3)}, Train Accuracy = {round(train_accuracy, 3)}\")\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Step {step}: Test Loss = {round(test_loss,3)}, Test Accuracy = {round(test_accuracy,3)}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"models/bert_step_{step}.pth\")\n",
    "\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3d237-3914-4eff-a27a-ce32efe66c36",
   "metadata": {},
   "source": [
    "# Generate the files for online submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6234a8-d42b-4250-95c2-2c7496da3f87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T23:30:20.609674300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiel\\AppData\\Local\\Temp\\ipykernel_9088\\1036872741.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n",
      "Processing validation_hidden.csv: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [00:05<00:00, 27.79it/s]\n",
      "Processing test_hidden.csv: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 143/143 [00:05<00:00, 28.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_predictions_csv(df, filename, model, tokenizer, device):\n",
    "    dataset = TextDataset(df, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {filename}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits).view(-1)\n",
    "            preds = (probs > 0.5).tolist()\n",
    "\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    df_predictions = pd.DataFrame(predictions) \n",
    "    df_predictions.to_csv(filename, index=False, header=False)\n",
    "\n",
    "generate_predictions_csv(validation_hidden_df, \"validation_hidden.csv\", model, tokenizer, device)\n",
    "generate_predictions_csv(test_hidden_df, \"test_hidden.csv\", model, tokenizer, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b53ff5-51db-46c3-b055-d026bea4af21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
